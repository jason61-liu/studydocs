 Q：GPT预训练cai yo采用了n哪些损失函数？
 A：模型预测值与真实标签之间的交叉熵
 Q：选择交叉熵而不是KL散度所谓损失函数？
 A：交叉熵就是在P分布下使用基于Q的最优编码方案的期望编码长度；KL散度 使用分布Q来表示P的时候丢失的信息量 ； 
 Q：通常在softmax层后使用交叉熵损失，为什么这种组合特别有效？
 A：softmax和交叉熵在分类任务中，简化梯度计算
 Q：大模型吞吐率过低怎么办
 A：吞吐率=处理请求N/延时，尽量减低模型的推理延迟，增大模型的并行处理请求的能力； 权重+激活量，提高缓存利用率，并行处理能力；模型的并行度；增大batch size，连续批处理


---

 Attention作用获取上下文信息，FFN的作用存储知识
 Norm使用LayerNorm，不是BatchNorm--训练
 都是用于对数据进行正则化，将输入数据归一至正态分布，加速收敛，提高训练稳定性
 BN：一个batch的向量，同一维度的数据做正则化；缺点：变长数据无法处理，语义数据无法处理；
 LN：序列向量中，不同时刻的向量做正则化
 ![](attachments/Pasted%20image%2020250915233839.png)
 运算量大n^2 ，加速算法kv-cache--推理阶段

preNorm和postNorm的区别：
![](attachments/Pasted%20image%2020250915233944.png)

multi-head self-attention中为什么要用三个不同的矩阵
 ![](attachments/Pasted%20image%2020250915235839.png)

