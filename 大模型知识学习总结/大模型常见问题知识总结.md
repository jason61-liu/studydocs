### 模型遗忘问题如何解决
- 数据层面：混合原始任务与新任务，数据混合策略，进行混合训练
- 训练策略：正则化约束/学习率控制/渐进式学习，冻结关键神经元权重，采用余弦退火策略
- 模型架构：多任务学习/Adapter模块/模型融合
- 评估监控：新旧任务指标同步验证，动态评估机制，设置平衡次数，增强原始数据权重，增加训练时间
### 多头自注意力机制
- 允许模型关注不同位置信息，从多个子空间捕获不同的特征
- 分头策略实现
- 矩阵变换实现
- 多视角特征捕捉：不同头侧重点关注特征不同
- 融合与正则化， 
- 工程优化flash attention
- 稀疏注意力，特征解耦，计算并行，灵活扩展，可解释路径
### RAG流程
- 检索增强生成，结合检索和生成两种技术，用于提升模型生成回答质量，特别需要外部知识的时候
- 用户输入处理-文档检索-文档处理-信息整合-生成回答-后处理
- 三个主要阶段：
	- 检索阶段
	- 增强阶段
	- 生成阶段

### 大模型微调流程
- 针对特定任务或领域进一步训练，使模型适应新的任务，如下步骤：
	- 确定任务和目标
	- 准备数据
	- 数据预处理
	- 选择预训练模型
	- 调整模型结构
	- 设置训练参数
	- 训练过程
	- 评估与验证
	- 部署与应用
### 模型微调评估效果
- 明确评估目标
- 技术指标评估
- 业务场景适配性：准确性，是否存在幻觉
- 效率与稳定性：效果，时延
	- 推理速度
	- 资源占用
	- 输出稳定性
- 长期监控和迭代：可观测性，roadmap
	- 在线指标监控
	- 数据漂移检测

### 超长文本进行微调训练
- 模型架构的改进：稀疏注意力机制，分块策略，
	- 稀疏注意力机制
	- 位置编码拓展，长度外推，让模型适应更长的文本
	- 
- 微调具体策略：Rope ALibi等相对位置编码
	- 分块与长文本保留，确保上下文连贯
	- 层次化处理
- 训练技巧与资源优化
	- 渐进式训练
	- 参数高效微调：lora，adapter
	- 显存优化技术：梯度检查点，混合精度训练，模型并行
- 任务驱动的微调策略
	- 注意力增强
	- 长文本目标函数设计
-评估与挑战
	-  评估指标
	- 信息碎片化
	- 显存限制
	- 位置外推
### 大模型应用保证实时性和多轮对话一致性
- 实时性优化
	- 模型优化：对模型进行蒸馏（知识蒸馏），保持性能的同时减小模型大小，从而加快推理速度；硬件加速；动态批处理
	- 缓存机制
	- 异步处理
	- 硬件加速
	- 响应分块
- 多轮会话一致性
	- 上下文管理：上下文窗口优化
	- 对话状态跟踪：维护一个对话状态机或上下文缓存
	- 显示确认和澄清
	- 模型微调：针对多轮对话任务对模型进行微调
	- 分块处理：将长对话分成多个片段

### 保证到模型生成内容的合规性
	- 技术层面：模型训练与部署关键控制，数据清晰和预过滤；RLHF；合规性微调；实时内容过滤；输出处概率阈值控制
	- 流程层面：构建审核闭环；多级审核机制；可解释性工具；版本灰度发布
	- 制度层面：合规体系搭建；法规 映射；权限分级控制；日志溯源与问责；第三方审计
	- 前沿技术补充：宪法式AI；数字水印

### 大模型实现多模态任务
- 处理多种类型数据，如文本，图像，音频，视频
- 图文问答，视频摘要
- 统一编码器
- 模态适配器
- 预训练-微调范式  
### 过拟合和欠拟合
- 过拟合：模型在训练数据表现极佳，但新数据集表现显著下降，过度训练导致，导致泛化能力比较差
	- 模型复杂度过高
	- 训练数据不足或噪声多
	- 训练时间过长
	- 特征冗余或无关特征过多
---
		- 增加数据量
		- 降低模型复杂度
		- 正则化技术
		- 早停法
		- 交叉验证
- 欠拟合：模型在训练和测试数据上表现不佳，无法捕捉数据中基本模式，模型过于简单或训练不足导致
	- 模型复杂度过低
	- 特征不足或缺乏代表性
	- 训练不充分
---
	- 增加模型复杂度
	- 特征工程
	- 减少正则化强度
	- 延长训练时间

### 全参数微调，lora，qlora区别
- 全参数微调：调整预训练大模型所有参数，使其适应下游任务
	- 资源需求：显存占用高，需要大量微调以避免过拟合
	- 效果：性能上限高，适合复杂任务；容易出现灾难性遗忘
- Lora：冻结元参数，旁路添加可训练低秩AB矩阵
	- 显存：新增参数量低
	- 训练速度快：更快，支持多任务匹配
	- 性能接近全参数微调，稳定性高，扩展性强
- QLora：引入量化技术 Lora+4bit量化源模型
	- 显存：节省资源
	- 计算代价：反量化操作增加训练时间
	- 效果：低显存下
### 模型蒸馏和模型量化
- 模型蒸馏：知识迁移为核心，牺牲少量精度换取模型轻量化
	- 将大型复杂模型的知识迁移到轻量的小模型，小模型模仿大模型的输出分布和中间特征
	- 软标签
	- 温度参数
	- 损失函数设计：软标签损失kl和原始标签交叉熵损失
	- 模型体积压缩至30M，推理速速提升5倍，准确率损失2%，满足业务需求
- 模型量化：降低计算精度提升效率，适合对延迟敏感场景
	- 将模型权重或激活值从高精度FP32转换为低精度INT8，降低计算和存储开销
	- 动态范围校准
	- 量化感知训练
	- 后训练量化
### vllm推理加速如何实现
- 内存优化 pageAttention，按需动态分配
- 连续批处理 continuous Batching，实时监控请求状态，动态处理请求，迭代级调度；抢占式推理
- 内存池：预先申请大块内存，避免频繁分配
- 吞吐量；平均延迟；GPU利用率
- ~~swap_space 参数，将部分kv缓存卸载到cpu内存~~
### 解决大模型幻觉问题
- 模型生成不准确或虚构信息，看似合理但不符合事实，过拟合，存在噪声，缺乏领域知识
- 事实性幻觉：RAG解决，知识增强
- 逻辑性幻觉：添加规则引擎校验 Logits偏置 正则表达式+业务关键词过滤
- 指令跟随偏差：强化STF数据指令对齐
### PPO和DPO区别
- PPO 近端策略优化，一种强化学习算法，常用于训练策略模型，通过限制策略更新幅度来确保稳定性
	- 目标：限制策略更新幅度最大奖励期望
	- 关键点：需训练奖励模型，通过策略-奖励互相迭代进行优化 KL散度
	- 计算复杂度高（多次策略-奖励模型交互）
	- 稳定性：需谨慎调参
	- 适用复杂奖励信号
- DPO 可能更直接利用人类便好数据来调整模型，不需要显示的奖励模型
	- 目标：直接利用人类偏好将策略对齐到偏好分布
	- 关键点：绕过奖励建模，直接通过偏好对（好答案vs坏答案）优化策略
	- 计算复杂度低
	- 更鲁棒（隐式约束策略偏离）
	- 明确偏好反馈
----
DPO在小规模高质量偏好数据数据下表现更优，且成本降低71%
PPO在需要细粒度奖励信号时（如区分满意度1-5分）仍有优势
核心区别在于如何利用反馈信号

数据量少但质量高--DPO
需要多维奖励信号--PPO
资源紧张--DPO

### 提示词工程能解决的问题
- 解决核心问题：让模型理解用户需求并精准完成任务
- 任务定义模糊性
- 输出格式控制
- 领域知识适配
- 偏见与安全性控制
- 复杂任务拆解
- 价值
	- 降低模型微调成本
	- 提升用户体验
	- 工程化扩展性
Bert --encoder-only
- 双向上下文建模
- 任务泛化性
- 训练和推理具有显著优势
- 需要创造性文本生成或多轮对话，decode-only更有优势，它在情感分析，知识问答存在优势
- 双向语义理解，推理速度快
chatGPT decoder-only 
- 核心特点：自回归
- 单流架构，无编码器组件
- 掩码自注意力机制
- 位置编码 绝对位置编码 旋转位置编码
- 文本连贯，创造性显著，内容创作，代码生成
- 生成质量高，训练效率优
---
encoder-only模型不需要掩码原因：
它主要处理完整且已知的输入序列，其核心任务是理解整个序列的上下文信息，为了充分捕捉词与词之间所有的方向的关系，它需要允许每个位置关注序列中的所有其他位置信息包括他后面的位置，施加掩码阻止关注未来位置会破坏这种全局立即能力
decode-only需要掩码原因：
在生成序列时，未来位置是未知的或不应该被访问的，掩码确保模型在预测当前位置t时，只依赖他之前已经确定的位置1到t-1，从而保持训练使用完整目标序列但掩码未来和推理逐步生成，没有未来信息的一致性

### flash attention
- 分块注意力计算，长序列按照固定长度切分，分块计算注意力并累加结果
	- 前向传播：每个chunk，仅计算内部及前序chunk的注意力，而非全局关联
	- 反向传播：分块反向计算梯度，释放未处理chunk的中间变量
- 重新计算厕率
- 内存访问优化
	- 平铺操作Tiling：将大矩阵乘法拆分多个小矩阵运算，适配GPU缓存结构，减少内存带宽瓶颈
	- 融合操作Fusion：将sofmax与矩阵乘法等操作合并为单个CUDA内核，减少内核调用开销
- 效率提升，训练速度；内存节省
- 保持数值精度
- 长序列处理能力拓展
- 大幅减少显存访问（I/O）
### MOE
- 混合专家模型，通过稀疏激活机制让模型具备分工协作的架构设计
- 路由网络，门控权重，选择激活概率最高的K个专家
- 负载不均衡 门控正交化+本地约束
- 专家协作不足：顺序激活专家链
- deepseek
	- 共享专家+256路由专家，细粒度任务分解
	- softmax门控改成sigmoid门控
	- 训练成本是传统MOE的十分之一

### 位置编码方式有哪些
- 为序列数据提供位置信息的关键技术
- 正余弦编码 通过三角函数线性组合，不依赖固定位置编码；长序列泛化能力

- 旋转位置编码RoPE
	- 通过复数旋转矩阵对Q和K进行位置依赖的旋转变化，将位置信息融入向量内积计算
- ALiBi（Attention with Linera Biases)
	- 不直接生成位置向量，而是在 注意力权重中添加与相对位置相关的偏置项，注意力偏置表示相对位置
	- 无需额外参数，计算高效，适合自回归生成任务
- 混合位置编码（绝对+相对）
	- 稀疏位置编码
	- 动态位置编码
### deepseek的多词预测方法MTP
- 密集训练信号加速学习：每个训练步骤中预测多个未来token（3-4个），讲训练信号密度提升数倍，同时提供多个token监督，加快模型收敛速度
- 参数共享和架构优化：共享Transformer主干和独立输出头，MTP损失函数通过多任务学习机制增强了模型的泛化能力，在相同数据量下实现更高的性能提升
- 上下文利用的深度增强
- 并行生成减少了解码步骤
- 推测芥末与验证修正结合
- 硬件资源的高效利用
- 场景
	- 长文本生成的连贯性优化
	- 推测解码和优化修正
### MCP（Model Contex Protocol）模型上下文协议
- 标准协议框架，统一AI模型与外部数据源，工具及系统的交互方式
- MCP主机host：负责发起请求并协调交互流程
	- 多服务器连接与动态能力交换
	- 安全数据访问机制
	- 统一的用户交互界面
- client
	- 初始化与连接管理
	- 工具调用与参数验证
	- 流式响应处理
- server
	- 资源与工具管理
	- 协议交互与会话控制
	- 开发与运维支持
### RAG文档分块
- 基于长度分块：按照预设token数或字符数阶段文档，不考虑语义边界
- 滑动窗口分块：固定块长度和重叠长度
- 基于标点符号/段落的分块
- 基于语义边界的分块
- 基于文档结构分块
- 递归分块，按结构分块，超长的块递归应用固定长度分块，确保不超过阈值
- 基于查询的动态分块：根据用户查询实时调整分块策略
- 知识图谱增强分块：结合知识图谱中实体关系，将文档按实体关联度分块
- 检索方式
	- 稀疏检索
	- 稠密检索
### 向量归一化
- 稠密检索
	- 确保相似度计算的合理性，稠密检索通常使用余弦相似度衡量相关性
	- 数值稳定性优化
	- 向量相似度计算可以简化为点积计算
### 标注训练数据低，如何扩增训练数据数量
- 数据增强：基于现有数据直接扩展，对原始数据进行合理变换
- 迁移学习与预训练模型，借用大模型的泛化能力
- 半监督学习：利用大量未标注数据
- 主动学习：智能选择有价值的样本标注

### RAG生成无关内容如何避免
- 精准查询理解与扩展
	- 增强查询语义表示 
	- 歧义消解与上下文感知
- 优化检索环节：提高文档相关性
	- 混合检索策略
	- 多轮检索与查询重写
- 文档预处理：减少噪声输入；智能分块与语义切分；文档过滤与质量控制
- 生成阶段：约束模型依赖检索内容；提示工程强化文档替换；降生成温度
- 引入反馈机制与迭代优化
	- 用户反馈与主动学习
	- 系统自评估与监控
- 数据与模型层面的深层优化
---
- 流程：结合检索和生成 
	- 检索阶段
	- 增强阶段
	- 生成阶段
用户输入处理-文档检索-文档处理-信息整合-生成回答-后处理
---

### 理解智能体的搭建
- 感知
- 决策
- 行动
- 记忆
---
- 任务定义与规划：
	- 目标解析
	- 规划方法
	- 工具选择
- 模型层
	- 核心模型
	- 角色设定
	- 微调需求：是否需要微调
- 工具库集成
	- 内置工具
	- 外部API
	- 自定义工具
- 记忆系统
	- 短期记忆
	- 长期记忆
	- 优先级策略
- 安全和可控性
	- 权限控制

### 大模型微调
- 在预训练好的基础模型上，针对特定任务或领域的数据进一步训练，使模型适应新的任务
	1. 确定任务和目标
	2. 准备数据：训练集，验证集和测试集
	3. 数据预处理：将数据转换成模型可以接受的格式
	4. 选择预训练模型：bert，gpt，t5
	5. 调整模型结构：全参数调整平衡
	6. 设置训练参数：学习率，批次大小，训练轮数，优化器等
	7. 训练过程：防止过拟合
	8. 评估与验证
	9. 部署与应用：持续监控性能，需要迭代优化
  - 如何评估效果
	  - 明确评估目标
	  - 技术指标：训练损失，验证损失，分类任务，生成任务；过拟合和欠拟合
	  - 业务场景适配：领域内测试，跨领域泛化
	  - 效率与稳定性：推理速度；资源占用；输出稳定性
	  - 长期监控与迭代：在线指标监控；数据漂移检测
### 超长文本训练微调
- 模型架构改进：稀疏注意力机制等；线性注意；分块处理与记忆机制
- 微调具体策略：RoPE，ALiBi，增加微调时的序列长度
- 数据预处理：文本分块，保持上下文相关性，确保信息不丢失，如滑动窗口；层次化处理
- 计算资源：需要更多显存资源；渐进式训练；LoRA，Adapter；梯度检查点；混合精度训练；模型并行即多卡并行
- 训练策略：先训练短文本，逐渐增加文本长度
- 挑战：信息碎片化，显存限制，位置外推

### 保证实时性和多轮对话
- 实时性优化
	-  模型优化：较小模型，模型蒸馏
	- 缓存机制：
	- 异步处理：
	- 硬件加速
	- 响应分块
- 多轮对话一致性
	- 上下文管理：历史对话记录作为输入传递给模型
	- 对话状态跟踪
	- 显示确认和澄清
	- 记忆机制
	- 模型微调：针对多轮对话任务行对模型微调
	- 分块处理
### 大模型生成内容的合规性
-  数据清洗与预过滤
- RLHF
- 合规性微调
- 实时内容过滤：敏感词匹配，文本相似度检测
- 输出概率阈值控制
- 多级审核机制
- 可解释性工具
- 版本会度发布，合规性指标，违规率，误拦截率等，避免系统性风险
- 法规映射
- 权限分级控制
- 日志溯源与问责
- 第三方审计
- 宪法式AI
- 数字水印
### 大模型多模态
- 多模态融合和联合建模
	- 模态异构性：文本，图像，视频 数据分布差异大，统一语义空间
	- 统一编码器
	- 模态适配器
- 训练策略
	- 预训练阶段
		- 对比学习
		- 掩码重建
		- 生成式预训练
	- 微调阶段
	- 提示词处理
- 挑战
	- 模态对齐不足
	- 长视频建模困难：分层处理
	- 数据稀缺：合成数据
- 端到端多模态大模型
- 具身多模态
- 低资源优化

### 降低大模型API服务推理延迟和成本
- 模型层面优化：量化，蒸馏，剪枝
- 推理过程优化：动态批次处理；请求优先级调度；异步推理
- 硬件层面优化：TPU，GPU等，不同模型使用不同硬件，混合精度训练；自动扩缩容
- 缓存策略：避免重复计算；结果缓存；请求去重；客户端预处理
- 架构设计上优化：模型并行，流水线并行，异步处理；边缘计算；微服务化
- 服务层面优化：动态资源调整，自动扩展
- 模型版本管理
- 网络优化：数据压缩；CDN/边缘节点
- 服务框架优化
- 用户层面策略：如限制请求频率，不同用户提供不同质量服务
---
### 全参数微调，Lora, QLora
- 全参数微调
	- 调整预训练大模型所有参数，使其适应下游任务
	- 资源需求：加载所有参数，显存占用高；需要大量微调数据以避免过拟合
	- 效果：理论性能上限高，适合复杂任务；容易导致遗忘
- Lora
	- 冻结原模型参数，旁路添加两个低秩矩阵，训练两个低秩矩阵，推理将结果与原模型相加
	- 资源需求：新增参数低；速度快
	- 效果：性能接近全参数；需要权衡参数量和效果
- QLora：
	- 引入量化技术，将原模型权重以4b精度存储，推理时反量化16bit或8bit
	- 资源需求：显存降低原来1/4；反量化会略微增加训练时间
	- 效果：性能一般



