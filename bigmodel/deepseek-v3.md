- 671B参数，激活37B参数 训练采用 H800进行训练
- 上下文窗口扩展至32K和128K
- 使用F8混合精读训练得到的大号MoE模型
- 并行策略
	- 64路专家并行，16路流水线并行，数据并行--训练
- 指标：
	- TFTF 首token生成时间，衡量prefill
	- TPOT 生成每个token时间，衡量 decode
- 特点
	 - Prefill：计算密集型，完成kv cache的生成后， 本身无需继续保留这些缓存
	 - Decode：访存密集型，需要尽可能多地保留缓存数据以及保障推理效率
- Attention:
	- TP=4 DP=8
	- MOE EP=32
	- 冗余专家 数量动态调整 prefill阶段 32个冗余阶段
- device-limited Routing:每个token最多放到Mge设备，首先选择最高得分专家所在的M个设备，进而降低通信成本
----
主要创新点
- MLA 多头潜在注意力机制
- DeepseekMoE
- 多token预测
- FP8训练混合精度训练：节约内存，减少通信传输，计算速度快