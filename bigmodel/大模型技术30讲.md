# 1.嵌入、潜空间和表征
## 1.1 嵌入
- 嵌入向量简称嵌入，能够将高维数据编码为低维向量
	- 独热编码：是一种将非类数据表征为二进制向量的编码方式，每个类别对应一个向量，该向量在分类索引对应的位置为1，其余位置取值为0，确保分类取值的表征形式可以被一些机器学习算法处理
	- 嵌入向量的维数可以比原始输入的维数更高，也可以更低，通过嵌入方法，可以将输入数据更极致的编码为二维码连续稠密向量的表征形式，用于可视化展示和聚类分析
## 1.2 潜空间
- 即嵌入空间同义，即嵌入向量被映射到的空间
	- 视为包含特征的任意特征空间，通常是输入特征的压缩版本
## 1.3 表征
- 输入的一种编码方式，通常是输入的中间形态

# 2.自监督学习
是一种预训练过程，能够让神经网络以监督学习的方式学习大规模无标签数据集
## 2.1自监督学习与迁移学习
- 迁移学习是先在一种任务上预训练模型，然后将预训练好的模型作为训练起点，然后再应用于第二种任务继续训练
   ![[Pasted image 20250726001553.png]]
- 自监督学习在无标签数据而非有变迁数据上进行模型的预训练，无标签数据集只能基于数据本身的结构想办法生成标签，自监督训练任务也被称为代理任务
- 迁移学习和自监督学习核心区别是如何获取标签
## 2.2自预测与对比自监督学习
- 自预测 通常会更改或掩蔽一部分内容，并训练模型来重建原始输入数据
   ![[Pasted image 20250726001449.png]]
   训练神经网络所产生的其嵌入向量，能够使得相似训练样本之间的向量距离最小化，不相似样本之间的向量距离最大化
   ![[Pasted image 20250726001347.png]]
# 3.小样本学习
小样本学习是一种监督学习方法，适用于训练集较小且每个标签的样本量都非常有限的情况
## 3.1数据集和术语
- 模型拟合训练数据集，并在测试数据集上对模型进行评测
- 小样本学习不是直接使用训练数据集，而是使用所谓支撑集
- 模型会从支撑集中抽样而成的训练任务上进行训练，每次训练任务完成，称之为一个回合
- 训练中遇到的标签被称为基类，支撑集通常被称为基集
- 元学习本质就是通过更新模型的参数，以便模型能够更好的适应新任务
# 4.彩票假设
## 4.1 彩票假设的训练过程

![[Pasted image 20250726002726.png]]
- 对单个权重剪枝称为非结构化剪枝
- 对网络总较大的块剪枝，比如整个卷集滤波通道，称为结构化剪枝
## 4.2实际意义和局限性

当目前为止，仍没有办法在不训练原始网络的情况下找到这些中奖彩票，如果算上剪枝步骤，这个训练过程甚至可能比常规成本还高

# 5.利用数据来减少过拟合现象
模型对训练数据拟合过于紧密，导致学习到了数据的噪声和异常值，而非数据背后真实的规律，结果就是在训练数据上表现良好，但是在未见过的数据或测试数据上表现不佳
## 5.1 常用方法
- 采集更多的数据：模型在训练集和验证集上表现的差距，反映过拟合程度---差距越大，说明过拟合越严重
- 数据增强：基于现有的数据生成新的数据样本或特征，它能在不采集额外数据的情况下扩充数据集
- 预训练
- 特征工程和标准化
- 加入对抗样本和标签或特征噪声
- 标签平滑
- 更小的训练批次
# 6.通过改进模型减少过拟合现象
## 6.1正则式
针对模型复杂度的惩罚机制，神经网络中的经典正则技术包括L2正则化及相关的权重衰减方法
- 在反向传播过程中，优化器尽可能最小化包含了额外惩罚项的损失函数，权重变小，有利于提升模型对未知数据的泛化性
1. Dropout会在训练期间随机将一些隐藏层单元的激活值为零，从而减少过拟合现象
2. 早停法会在训练时密切观察模型在验证集上的表现，一旦发现模型在验证集上性能性能开始下降，立即停止训练
- 经典偏差-方差理论指出，缩小模型规模也可以减少过拟合，一般情况下模型参数越少，模型记住或者过度拟合数据中的噪声的能力也减弱
- 迭代剪枝就是先训练一个模型，在原始数据集上取得不错的性能后，再逐步去除模型参数，并在数据集上重新训练，训练应当保持模型的预测性能与原模型相同
- 知识蒸馏：将一个大而复杂的模型的知识转移到一个更小的模型
- 集成方法就是将多个模型的预测结果合到一起，提高整体的预测效果，多个模型的代价会让计算成本增加
- k折交叉验证：一种在k个训练子集上分别进行训练和验证模型的评测方法，对迭代性能指标的均值进行k次计算，来评测模型的整体表现，k折集成方法每次使用k-1个训练子集进行训练，最终得到k个模型
# 7.多GPU训练方式
## 7.1 训练模型
- 模型并行：也称操作并行，是一种将大模型的不同部分放到不同的GPU上按序计算的技术，计算的过程数据会在不同设备间传递。应对GPU显存限制的优秀策略，尤其是在完整网络无法适配单GPU的情况下，缺陷就是GPU之间必须互相等待，从而不能高效并行工作，因为他们互相依赖彼此输出
- 数据并行：小批量数据划分到更小的微批量数据，然后让每个GPU分别处理一个微批量数据，并计算模型权重的损失和梯度，每个GPU都需要模型的一份完整副本，如果模型大到无法放进GPU,数据并行则不可用
- 张量并行：操作内并行，一种更高效的模型并行方式，将权重和激活矩阵拆分到各个设备上，而非将不同的模型层分散到各个设备上，矩阵拆分使得将举证乘法分布到多个GPU上执行，多个GPU之间进行矩阵分割的通信开销变高
- 模型并行策略解决单GPU显存限制的问题，数据并行将批量数据拆分到多个GPU中，以解决无法并行训练的问题，数据并行会计算梯度的平均值来更新权重，张量并行则是模型大到无法完整装载斤单GPU显存是，降输入的矩阵拆分到不同GPU处理
- 流水线并行：与模型并行乐死，会在钱箱传播时将激活值传递下去，独特之处在于在反向传播过程中，输入张量的梯度会被回传回来，防止设备空闲，目的最小化串行计算的性能瓶颈，增强设备在不同设备上的模型层的并行性
- 序列并行：处理长序列时会碰到计算瓶颈问题，该方案将输入序列分割成更小的块，拆分到不同的GPU上，从而减少自注意力机制对计算内存的需求，可能会降低模型的准确性
# 8.Transformer架构的成功
## 8.1注意力机制
编码器通过自注意力机制来计算序列中每个输入词元相对于其他词元的重要程度，从而使得模型能够聚焦于输入序列中相关联的部分
注意力机制使得神经网络有选择地判断不同的输入特征的重要性，从而使模型在面对特定任务时聚焦于输入中文关联性最强部分
## 8.2通过自监督学习进行训练
通过大规模无标签数据集上进行自监督学习，实现Transformer预训练
## 8.3大规模参数
## 8.4轻松并行化
# 9.生成式AI模型
## 9.1生成式模型与判别模型
- 生成式模型的目标是捕获输入数据的概率分布p(x)，或输入数据与输出标签之间联合概率分布p(x,y)
- 判别式模型则专注于给定输入的条件下，对输出的标签条件概率分布p(x|y)进行模型建模
逻辑回归是判别式模型，朴素贝叶斯是生成式模型
## 9.2 深度生成式模型的类型
### 9.2.1能量模型
EBM（energy-based model）属于生成模型中的一种，通过学习能量函数来为每个数据点分配一个标量值，能量值越低，所对应数据点出现的可能性越高，模型目标是在训练过程总让真实数据点的能量值最小化，同时增加生成数据点的能量值
- DBM（deep Blotizmann machine，深度波尔茨曼机）包含多层隐藏节点，通过吉布斯采样从分布中进行抽样
- MLP（multilayer prerceptron，多层感知机）
### 9.2.2变分自编码器
VAE(variatinal autoencoder)将变分推理与自编码器架构融合，变分推理通过对更简单，可处理的分布进行优化，使其尽可能接近真实的复杂概率分布，从而得到复杂概率分布的近似。自编码器是一种无监督的神经网路，将输入数据压缩成更低维的表征，然后通过最小化重建数据的误差，从低维度中重新构造出原始数据
重建损失用于确保解码器生成的样本与输入图像尽可能相似，而KL散度正则作为一种替代损失，促使学习到潜分布接近预设的先验分布
### 9.2.3生成对抗网络
由两个神经网络组成，生成器和判别器，通过对抗的方式同时训练，缺点是损失函数及旭熙过程中对抗性质导致训练不稳定
![[Pasted image 20250726143207.png]]
### 9.2.4流模型
又称为归一模型，核心理念源自于统计学中经典方法，目的是通过可逆变换，将简单的概率分布转变为更为复杂的分布
- NICE（non-linear independent components estimation 非线性独立分量估计）将复杂的输入分布映射为更简单的分布，然后再映射回来
流模型能够提供精确的概率度量
![[Pasted image 20250726144040.png]]
### 9.2.5自回归模型
根据当前及过去的内容预测下一个值，典型示例文本生成的LLM，生成新样本时的速度较慢，训练难度低
### 9.2.6扩散模型
利用随机微分方程，通过一系列步骤将输入数据的分布转化为简单的噪声分布，它是一种随机过程，其逐步向数据中添加噪声，直至接近更简单的分布，如高斯分布，为了能生成新样本，这一过程还会反向生成，即从加入噪声的分布开始，逐步减轻噪声的影响，它在生成高质量图像、逼真的细节以及纹理等方面达到了顶尖水平
### 9.2.7一致性模型
一致性模型训练网络将还有噪声的图像映射为更清晰的图像，该神经网络实在一个包含成对含噪声图像和清晰图像的数据集上训练的，它会学习识别在清晰图像中加噪的模式，一旦神经网络训练完成，即可基于含噪声图像进一步生成其重建图像
# 10.随机性的由来
## 10.1模型权重初始化
pytorch的所有主流神经网络框架，默认都会在模型的每一层随机初始化权重值和偏置单元，可以通过随机数生成种子值，使随机的初始化权重变为确定的
![[Pasted image 20250726145819.png]]
## 10.2数据集采样和重排
数据集分为训练集和测试集以及验证集
## 10.3非确定性算法
在模型中融入随机的组件和算法
Dropout会在训练过程中随机将一层中的部分单元提出，帮助模型学习到稳健和泛化性更好的特征，这种剔除操作通常在每次训练迭代时以概率p发生，其中p是一个控制剔除单元比例的超参数，p一般是0.2到0.8
同时为了确保训练过程中可浮现，必须在启用dropout之前，先设定随机种子
## 10.4不同运行时的算法
- 经典的直接卷积
- 基于FFT（fast fourier transform 快速傅立叶变换）卷积
- Winograd卷积
## 10.6随机性与生成式AI
- top-k采样：在生成下一个词过程中，每一步都从概率最高的前k个候选词中进行采样
- top-p采样（核采样）：基于一个概率阈值p来选择词，他会按照概率降序累积最可能的词，直到他们的累积概率达到或超过阈值p
***
# 11.计算参数量
## 11.1卷积层
## 11.2全连接层
![[Pasted image 20250726151740.png]]
# 12.全连接层和卷积层
# 13.ViT架构所需的大型训练集
## 13.1CNN中的归纳偏置
- 局部连接：在CNN中，隐藏层的每个单元仅与前一层中的部分神经元相连
- 权重共享：通过卷积层，整个图像中使用同一小组权重
- 分层处理：CNN由多个卷积层组成，用于从出入图像中提取特征
- 空间不变性：即时输入信号在空间域内移动到不同的位置，模型的输出也保持一致性
平移不变性即在空间域内对输入信号进行位移或平移后，输出保持不变
平移等变性意味着输出会随着输入以响应的方式发生位移

多层感知机这类全连接网络则不具备这种空间不变性或等变性
## 13.2ViT中的归纳偏置
将输入图像切分成多个块，以便独立处理每个输入块，在此过程中，每个块都能关注到其他所有块，从而使模型能够学习输入图像中相隔较远的块之间的关系
通过这种份块归纳偏置，ViT能够在不增减模型参数量的情况下处理更大尺寸的图像，从而避免计算成本过高
****
# 14.分布假设
## 14.1
- Word2vec：通过简单的两层网络，将词语编码为嵌入向量，确保相似词语的嵌入向量在语义和句法上也相近。训练Word2vec两种方式：CBOW（continuous bag-of-words连续词袋）模型和跳字（skip-gram）模型
- Bert采用掩码建模方法，句子中隐藏部分词语，根据序列中的其他词语预测这些隐藏词语，属于大模型训练的自监督学习方法，经过训练，相似的词语或词元在嵌入空间中的距离更近
- GPT侧重于预测后续内容，严格只依据前面的序列元素进行解析
# 15.文本数据增强
## 15.1同义词替换
帮助模型理解不同词语间可能存在的相似含义，从而提升模型的理解和生成文本能力
## 15.2词语删除
通过随机删除训练数据中的词，让模型学会即时在部分信息缺失的情况下也能做出准确的预测，帮助模型在现实世界遇到不完整或者含有噪声的数据时更具有稳健型
## 15.3词语位置交换
也称为词混排或词排列，通过改变或交换句子中的词语位置，生成句子的新变体
## 15.4句子乱序
通过对文档中句子进行乱序重拍，时模型接触到相同内容的不同排列方式，帮助他学习识别主题要素和关键概念，而不是依赖特定的句子顺序，有助于模型更深入理解文档的主题或文档类型
## 15.5噪声注入
- 随机插入字符
- 随机删除字符
- 引入打字错误
## 15.6回译
先将原文翻译成一种或多种不同的语言，然后再翻译回原语言
## 15.7合成数据生成
用于描述哪些人造数据的方法或技术，包括仿真数据和复制现实世界结构的数据
# 16.自注意
## 16.1RNN中的注意力
处理长序列的注意力机制实例之一，偏向于时间序列任务
## 16.2自注意机制
注意力机制应用于同一个序列的所有元素之间，上下文向量就是对输入序列元素的注意力加权求和得到的
# 17.编码器和解码器风格的Transformer架构
## 17.1原始的Transformer
输入文本首先被切分为独立的词元，随后通过嵌入层进行编码，再进入编码器阶段，再给么哥嵌入的词添加位置编码向量后，这些嵌入向量通过多头自注意力层，执行层归一化，并同哟残差连接将原始嵌入向量加回，接下来是层归一化模块，通过对前一层的激活值进行归一化来提高神经网络训练的稳定性，最后进入全连接网络，由两个全连接层组成的小型多层感知机，期间有个非线性激活函数再次加和并归一化，随后再传递给解码器的多头自注意层
- 编码器：BERT是一种基于Transformer编码器模块的纯编码器架构，掩码语言建模思想就是在输入序列中随机掩蔽或替换词元，然后训练模型根据上下文预测原本掩蔽的词，能够学习输入文本的丰富上下文表征，然后就可以对各种下游任务进行微调，如情感分析，问答和命名实体识别
- 解码器：多头自注意机制与编码器中类似，但在解码器中会被屏蔽，以防止模型注意到接下来的位置，今儿确保位置i的预测结果只能依赖小雨i的位置的已知输出
- 编码器-解码器混合模型：通常应用于涉及理解输入序列并生成输出序列的自然语言的处理任务，这些序列通常有不同的长度和结构
# 18.使用和微调训练Transformer
基于特征的方法，上下文提示词和更新模型参数的子集
## 18.1使用Transformer执行分类任务
- 基于特征方法中，加载预训练模型，并使其保持“冻结”状态，即不更新预训练模型的任何参数
- 微调预训练大模型的传统方法包括只更新输出层，我们称之为微调I，以及更新模型的所有层，我们称之为微调II
## 18.2上下文学习、索引和提示词调优
零样本学习或小样本学习
## 18.3参数高效的微调方法
- 软提示词调优：在嵌入化的查询词元前添加一个可训练的参数张量，通过梯度下降调整潜质的张量，来提升在目标数据集上的夹膜性能
- 前缀调优：讲可训练的张量加在每个Transformer模块的前面，而不仅仅是嵌入的输入前，可以保持训练过程的稳定
- 适配器方法：使用原始的适配器方法训练大模型时，只有新的适配器会更新，而其余的Transformer则保持冻结状态
   ![[Pasted image 20250726171755.png]]
   - 低秩自适应（LoRA）:是一种使用低维表征来近似高维矩阵或数据的技术，低维表征是通过找到可以有效捕获掩饰数据中大部份信息的低维度组合实现的
## 18.4 基于人类反馈的强化学习
预训练模型通过监督学习和强化学习的结合进行微调，PPO近端策略优化
## 18.5适配预训练语言模型

# 19.评测生成式大模型
## 19.1大模型评测指标
- 困惑度：通常永固评测文本生成模型和文本补全模型，本质上量化模型在给定上下文中预测下一个词的平均不确定性--困惑度越低，模型的表现越好
- BLEU：将模型的输出与人工参考文本进行比较，通过统计生成文本中有多少词出现在参考文献中，并除以候选文本长度，衡量字符串的相似度
	- 它衡量字符串相似度，而仅凭相似度不足以全面捕捉翻译质量
- ROUGE：对文本摘要进行评分的流行指标，未考虑到同义词或意译的情况
- BERTScore：
***
# 20.无状态训练与有状态训练
## 20.1无状态训练
先在原始训练集上训练初始模型，然后每当有新数据到来时便重新训练模型，因此也称为无状态重训练
## 20.2有状态训练
更新模型之能应对概念、特征和标签的偏移，优先就是无须为了重新训练而存储数据，相反可以立即使用新的数据来更新模型
# 21.以数据为中心的人工智能
# 22.加速推理
## 22.1并行化
将模型应用于一批样本，而不是一次只处理一个样本，也称为批量处理
## 22.2向量化
指单个步骤中对整个数据结构进行操作，而不是使用for循环这样的迭代结构
## 22.3循环分块
通过将循环的迭代空间分解为更小的部分或小块来增强数据局部性
## 22.4算子融合
又称为循环融合，是一种将多个循环合并为单个循环的优化技术，比如将原本分别计算一组数字的总和和乘机的两个独立循环融合一个单一的循环
算子融合可以通过提升模型的性能：减少循环控制的开销，提高缓存性能以减少内存访问时间，以及向量化实现进一步的优化
算子融合相关概念是重新参数化，用于将多个操作简化为单一操作
## 22.5量化
降低机器学习模型的计算和存储需求，在训练量化中，模型首先通过全精度权重进行正常训练，在训练完成后再进行量化。量化感知训练则在训练过程中引入量化步骤，使得模型能够在学习到补偿量化影响的能力，有助于保持模型的准确性
# 23.数据分布偏移
训练模型所用的数据的分布与模型在现实世界中遇到的数据的分布之间的差异
## 23.1 协变量偏移
当输入数据的分布p(x)发生变化，但给定输入下输出的条件分布p(y|x)保持不变，就会发生协变量偏移
检测协变量迁移一种常见技术对抗性验证，一旦检测到协变量迁移，使用重要性加权
## 23.2标签偏移
也称为先验概率偏移 发生在分类标签分布p(y)发生变化而条件类别分布p(y|x)保持不变 情况下
解决标签偏移问题使用甲醛损失函数更新模型
## 23.3概念偏移
输入特征与目标变量之间映射关系的变化
## 23.4领域偏移
## 23.5数据偏移类型

***
# 24.泊松分布
当数据是计数类型时，使用柏松回归，，当知道某些结果比其他结果高或低，但不确定它有多重要，甚至不确定它是否重要时，使用有序回归
# 25置信区间
## 25.1 定义置信区间
置信区间是一种估计未知总体参数的方法。总体参数是统计总体的某一个特定度量
## 25.2方法
- 方法1:正态近似分布 通过一次训练集-测试集的划分来构建置信区间，缺点 正态近似并不总是准确，特别是小样本对于小样本或非正态分布的数据而言
- 方法2:使用自助法构建训练集
- 方法3:使用自助抽样法构建测试集预测结果
- 方法4:使用不同的随机种子重新训练模型
# 26 置信区间与共形预测
## 26.1 置信区间与预测区间
- 置信区间侧重于刻画总体特征的参数，预测区间则为单个预测魔表提供一个值域
## 26.2 预测区与共形预测
二者都是用于估测单个模型预测的不确定性的统计技术，预测区间通常假设特定的数据分布，并且与特定类型的模型相关，而共形预测方法不受分布限制，可以应用于任何机器学习算法
## 26.3预测区域、预测区间与预测集合
在回归任务中，输出是连续变量，预测区间提供了一个范围，在一定置信水平下，真实值预计将落入系范围
在分类任务中，输出是离散变量（分离标签），预测集合包含了给定实例所有可能预测结果的分类标签
## 26.4计算共形预测
共形预测方法提供了一个用于创建预测区域的框架，即预测任务的潜在结果集合
# 27.合适的模型度量
- 标准
- 均方误差：用于计算目标变量与预测的目标值之差的平方的平均值
- 交叉熵损失：用于测量两个概率分布之间的距离，在机器学习的背景下，当我们使用由n个训练样本组成的数据集训练逻辑回归或神经网络分类器时，使用分类标签与预测概率之间的散交叉熵损失
# 28.k折交叉验证中的k
- k折交叉验证视为进行模型评测的一种方法，模型的泛化性能，即模型在新数据上的表现如何
- k的常见选择时5或10，k值过大，不同轮次的交叉验证的训练集会过于相似
# 29.训练集和测试集的不一致性
- 对抗验证时一种识别训练数据和测试数据相似程度的技术
# 30.有限的标签数据
- 标注更多的数据
- 自助抽样数据
- 迁移学习
- 自监督学习（无监督预训练）
- 主动学习：模型主动选择数据进行打标
- 小样本学习
- 元学习：重点是学习出一个好的特征提取模块，该模块将支撑集和查询图像转换为向量表征，通过与支撑集中的训练样本进行比较，优化这些向量表征，从而确定查询样本的预测分类
- 弱监督学习：弱监督标签函数创建的标签比人类或领域专家打的标签更糟杂或更不准确
- 半监督学习：不实用外部标签函数，而是利用数据本身的结构
- 自训练
- 多任务学习
- 多模态学习
- 归纳偏置
