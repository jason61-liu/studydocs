```shell
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen3-0.6B",
        "prompt": "1+5等于几",
        "max_tokens": 7,
        "temperature": 0
    }'

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

huggingface-cli download --resume-download Qwen/Qwen3-0.6B --local-dir Qwen3-0.6B

huggingface-cli download --resume-download Qwen/Qwen2.5-1.5B-Instruct --local-dir Qwen2.5-1.5B-Instruct

curl http://localhost:8001/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "prompt": "Beijing is a",
        "max_tokens": 7,
        "temperature": 0
    }'


python -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8001 \
    --model Qwen/Qwen2.5-1.5B-Instruct \
    --trust-remote-code

curl http://localhost:8001/v1/models



export PATH=/usr/local/cuda-12.8/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH


sudo docker pull vllm/vllm-openai:latest


export DOCKER_CLIENT_TIMEOUT=6000
export COMPOSE_HTTP_TIMEOUT=6000

sudo docker run -d -t --net=host --gpus all \
 --privileged \
 --ipc=host \
 --name vllm \
 -v /root:/root \
 vllm/vllm-openai:latest


python3 -m vllm.entrypoints.openai.api_server \
--model /root/Qwen3-0.6B \
--trust-remote-code \
--tensor-parallel-size 1

python -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8001 \
    --model /root/Qwen3-0.6B 

curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "/root/Qwen2.5-7B-Instruct",   
    "messages": [
    {"role": "system", "content": "你是个友善的AI助手。"},
    {"role": "user", "content": "介绍一下什么是大模型推理。" }
    ]}'


curl http://8.145.33.205:8001/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "/root/Qwen3-0.6B",   
    "messages": [
    {"role": "system", "content": "你是个友善的AI助手。"},
    {"role": "user", "content": "介绍一下什么是大模型推理。" }
    ]}'

curl http://8.145.33.205:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "model": "/root/Qwen2.5-7B-Instruct",   
    "messages": [
    {"role": "system", "content": "你是个友善的AI助手。"},
    {"role": "user", "content": "介绍一下什么是大模型推理。" }
    ]}'


docker run --runtime nvidia --gpus all --name vllm-0.6B -v /root/Qwen3-0.6B:/root/Qwen3-0.6B -p 8001:8000 --ipc=host vllm/vllm-openai:latest --model /root/Qwen3-0.6B --trust-remote-code


docker run --rm  --runtime nvidia --gpus all -v /root/Qwen2.5-7B-Instruct:/root/Qwen2.5-7B-Instruct -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model /root/Qwen2.5-7B-Instruct --trust-remote-code --enable-prefix-caching



curl http://8.145.33.205:8000/v1/metrics
```